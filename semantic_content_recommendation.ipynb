{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "figured-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Semantic Content Recommendation System (WIP)\n",
    "\n",
    "# There are some issues with this lab:\n",
    "# https://aws.amazon.com/getting-started/hands-on/semantic-content-recommendation-system-amazon-sagemaker/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cross-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download and Prepare the Dataset\n",
    "\n",
    "# import libraries\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import sagemaker\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.datasets.twenty_newsgroups import strip_newsgroup_header, strip_newsgroup_quoting, strip_newsgroup_footer\n",
    "\n",
    "# use SageMaker 1.x for this lab\n",
    "if int(sagemaker.__version__.split('.')[0]) == 2:\n",
    "    !pip install sagemaker==1.72.0 -U\n",
    "    print(\"Installing previous SageMaker Version. Please restart the kernel\")\n",
    "else:\n",
    "    print(\"SageMaker version is good\")\n",
    "\n",
    "# define environment variables\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')['data']\n",
    "newsgroups_test = fetch_20newsgroups(subset = 'test')['data']\n",
    "NUM_TOPICS = 30\n",
    "NUM_NEIGHBORS = 10\n",
    "BUCKET = 'BUCKET'\n",
    "PREFIX = '20newsgroups'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-stranger",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.a. Preprocess the data\n",
    "\n",
    "# in NLP, preprocessing raw text data means converting it into machine-readable numeric values\n",
    "# this involves a sequence of steps\n",
    "\n",
    "# strip any headers, footers, and quotes from the dataset\n",
    "for i in range(len(newsgroups_train)):\n",
    "    newsgroups_train[i] = strip_newsgroup_header(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_quoting(newsgroups_train[i])\n",
    "    newsgroups_train[i] = strip_newsgroup_footer(newsgroups_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grateful-stake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view one of the training examples\n",
    "newsgroups_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.b. Perform token counting\n",
    "\n",
    "# to be machine-readable, assign a token for each word, up to the most common 2000\n",
    "# less frequent words will have a diminishing impact on the overall model and will be ignored\n",
    "\n",
    "# for each document, use a Bag of Words (BoW) model to vectorize the number of times each token appears\n",
    "# WordNetLemmatizer is the tokenizer\n",
    "# CountVectorizer performs token counting\n",
    "\n",
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "token_pattern = re.compile(r\"(?u)\\b\\w\\w+\\b\")\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) if len(t) >= 2 and re.match(\"[a-z].*\",t) \n",
    "                and re.match(token_pattern, t)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-history",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary libraries\n",
    "import time\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vocab_size = 2000\n",
    "print('Tokenizing and counting, this may take a few minutes...')\n",
    "start_time = time.time()\n",
    "vectorizer = CountVectorizer(input='content', analyzer='word', stop_words='english',\n",
    "                             tokenizer=LemmaTokenizer(), max_features=vocab_size, max_df=0.95, min_df=2)\n",
    "vectors = vectorizer.fit_transform(newsgroups_train)\n",
    "vocab_list = vectorizer.get_feature_names()\n",
    "print('vocab size:', len(vocab_list))\n",
    "\n",
    "# random shuffle\n",
    "idx = np.arange(vectors.shape[0])\n",
    "newidx = np.random.permutation(idx) # this will be the labels fed into the KNN model for training\n",
    "# Need to store these permutations:\n",
    "\n",
    "vectors = vectors[newidx]\n",
    "\n",
    "print('Done. Time elapsed: {:.2f}s'.format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "julian-latitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.c. Stage the Training and Validation Datasets in S3\n",
    "\n",
    "# convert the vectors to a sparse representation\n",
    "import scipy.sparse as sparse\n",
    "vectors = sparse.csr_matrix(vectors, dtype=np.float32)\n",
    "print(type(vectors), vectors.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-intake",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert data into training and validation data\n",
    "n_train = int(0.8 * vectors.shape[0])\n",
    "\n",
    "# split train and test\n",
    "train_vectors = vectors[:n_train, :]\n",
    "val_vectors = vectors[n_train:, :]\n",
    "\n",
    "# further split test set into validation set (val_vectors) and test  set (test_vectors)\n",
    "\n",
    "print(train_vectors.shape,val_vectors.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bibliographic-terminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the paths in S3 to store training data, validation data, and NTM artifacts\n",
    "\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "\n",
    "train_prefix = os.path.join(prefix, 'train')\n",
    "val_prefix = os.path.join(prefix, 'val')\n",
    "output_prefix = os.path.join(prefix, 'output')\n",
    "\n",
    "s3_train_data = os.path.join('s3://', bucket, train_prefix)\n",
    "s3_val_data = os.path.join('s3://', bucket, val_prefix)\n",
    "output_path = os.path.join('s3://', bucket, output_prefix)\n",
    "print('Training set location', s3_train_data)\n",
    "print('Validation set location', s3_val_data)\n",
    "print('Trained model will be saved at', output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-involvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NTM supports CSV and RecordIO, so this helper function formats\n",
    "# the data into RecordIO\n",
    "\n",
    "# 'n_parts' optionally shards the dataset, allowing for distributed training\n",
    "def split_convert_upload(sparray, bucket, prefix, fname_template='data_part{}.pbr', n_parts=2):\n",
    "    import io\n",
    "    import boto3\n",
    "    import sagemaker.amazon.common as smac\n",
    "    \n",
    "    chunk_size = sparray.shape[0]// n_parts\n",
    "    for i in range(n_parts):\n",
    "\n",
    "        # Calculate start and end indices\n",
    "        start = i*chunk_size\n",
    "        end = (i+1)*chunk_size\n",
    "        if i+1 == n_parts:\n",
    "            end = sparray.shape[0]\n",
    "        \n",
    "        # Convert to record protobuf\n",
    "        buf = io.BytesIO()\n",
    "        smac.write_spmatrix_to_sparse_tensor(array=sparray[start:end], file=buf, labels=None)\n",
    "        buf.seek(0)\n",
    "        \n",
    "        # Upload to s3 location specified by bucket and prefix\n",
    "        fname = os.path.join(prefix, fname_template.format(i))\n",
    "        boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "        print('Uploaded data to s3://{}'.format(os.path.join(bucket, fname)))\n",
    "split_convert_upload(train_vectors, bucket=bucket, prefix=train_prefix, fname_template='train_part{}.pbr', n_parts=8)\n",
    "split_convert_upload(val_vectors, bucket=bucket, prefix=val_prefix, fname_template='val_part{}.pbr', n_parts=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "violent-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Train and Deploy the topic model with NTM\n",
    "\n",
    "# NTM learns inferred topics from documents using observed word distributions\n",
    "# in the document corpus. Semantics are usually determined by examining the top\n",
    "# ranking words they contain.\n",
    "\n",
    "# 2.a. Create and run the training job\n",
    "import boto3\n",
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "# built-in SageMaker algorithms are stored as Docker containers in ECR\n",
    "# the following code pulls the 'ntm' image in the current region\n",
    "container = get_image_uri(boto3.Session().region_name, 'ntm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thousand-brown",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the SageMaker Estimator allows for specifying the training infrastructure:\n",
    "# - instance type\n",
    "# - number of instances\n",
    "# - hyperparams\n",
    "# - output path\n",
    "# - etc...\n",
    "sess = sagemaker.Session()\n",
    "ntm = sagemaker.estimator.Estimator(container,\n",
    "                                    role, \n",
    "                                    train_instance_count=2, \n",
    "                                    train_instance_type='ml.c4.xlarge',\n",
    "                                    output_path=output_path,\n",
    "                                    sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-immune",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparameters for the topic model\n",
    "ntm.set_hyperparameters(num_topics=NUM_TOPICS, feature_dim=vocab_size, mini_batch_size=128, \n",
    "                        epochs=100, num_patience_epochs=5, tolerance=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twelve-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify how the training instances will handle the dataset\n",
    "from sagemaker.session import s3_input\n",
    "\n",
    "# distribution='ShardedByS3Key' splits the full dataset among the workers\n",
    "# distribution='FullyReplicated' would have all data files copied to all workers\n",
    "s3_train = s3_input(s3_train_data, distribution='ShardedByS3Key') \n",
    "ntm.fit({'train': s3_train, 'test': s3_val_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attended-nomination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b. Deploy the Topic Model\n",
    "\n",
    "# a trained model by itself is a tar file consisting of the model\n",
    "# weights and does nothing on its own. There are 2 ways to deploy\n",
    "# the model on SageMaker to invoke predictions on input.\n",
    "\n",
    "# Option A - Hosted Endpoint - One inference at a time\n",
    "# Option B - Batch Transform - Inferences for up to the entire dataset at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-television",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b. - Option A - Hosted Endpoint\n",
    "\n",
    "# deploy from the estimator\n",
    "# specify the number on type of instances to host the endpoint\n",
    "\n",
    "# creates a deployable model, configures the endpoint, and launches\n",
    "# the endpoint to host the model\n",
    "ntm_predictor = ntm.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-sewing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to run inferences against the endpoint\n",
    "# CSV as input\n",
    "# JSON as output\n",
    "from sagemaker.predictor import csv_serializer, json_deserializer\n",
    "\n",
    "ntm_predictor.content_type = 'text/csv'\n",
    "ntm_predictor.serializer = csv_serializer\n",
    "ntm_predictor.deserializer = json_deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-cooking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the topic vectors for the training data that will be used in the KNN model\n",
    "\n",
    "predictions = []\n",
    "for item in np.array(vectors.todense()):\n",
    "    np.shape(item)\n",
    "    results = ntm_predictor.predict(item)\n",
    "    predictions.append(np.array([prediction['topic_weights'] for prediction in results['predictions']]))\n",
    "predictions = np.array([np.ndarray.flatten(x) for x in predictions])\n",
    "\n",
    "# below doesn't work\n",
    "# where is 'train_labels' coming from?\n",
    "# where is 'categories' coming from?'\n",
    "\n",
    "# topicvec = train_labels[newidx]\n",
    "# topicnames = [categories[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charitable-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-vector",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-break",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "entire-regression",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interesting-italian",
   "metadata": {},
   "outputs": [],
   "source": [
    "newidx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powered-lover",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "military-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the topic names\n",
    "topicvec = np.argmax(predictions, axis=1)\n",
    "topicnames = [vocab_list[x] for x in topicvec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "several-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b. - Option B - Batch Transform\n",
    "\n",
    "# run inferences against a batch of data at a time, then SageMaker tears\n",
    "# down the resources once completed\n",
    "\n",
    "np.savetxt('trainvectors.csv',\n",
    "           vectors.todense(),\n",
    "           delimiter=',',\n",
    "           fmt='%i')\n",
    "batch_prefix = '20newsgroups/batch'\n",
    "\n",
    "train_s3 = sess.upload_data('trainvectors.csv', \n",
    "                            bucket=bucket, \n",
    "                            key_prefix='{}/train'.format(batch_prefix))\n",
    "print(train_s3)\n",
    "\n",
    "# specify the output path for the batch transform job\n",
    "batch_output_path = 's3://{}/{}/test'.format(bucket, batch_prefix)\n",
    "\n",
    "# specify the workers to perform the batch transform job\n",
    "ntm_transformer = ntm.transformer(instance_count=1,\n",
    "                                  instance_type ='ml.m4.xlarge',\n",
    "                                  output_path=batch_output_path\n",
    "                                 )\n",
    "ntm_transformer.transform(train_s3, content_type='text/csv', split_type='Line')\n",
    "ntm_transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the output back to this local notebook instance \n",
    "# to inspect the topic model's results\n",
    "\n",
    "!aws s3 cp --recursive $ntm_transformer.output_path ./\n",
    "!head -c 5000 trainvectors.csv.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-saturn",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.c. Explore the Topic Model\n",
    "\n",
    "# Visualize the topic vectors in 2D space\n",
    "from sklearn.manifold import TSNE\n",
    "import pandas as pd\n",
    "time_start = time.time()\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=50, n_iter=5000)\n",
    "tsne_results = tsne.fit_transform(predictions)\n",
    "print('t-SNE done! Time elapsed: {} seconds'.format(time.time()-time_start))\n",
    "tsnedf = pd.DataFrame()\n",
    "tsnedf['tsne-2d-one'] = tsne_results[:,0]\n",
    "tsnedf['tsne-2d-two'] = tsne_results[:,1]\n",
    "tsnedf['Topic']=topicnames\n",
    "plt.figure(figsize=(25,25))\n",
    "sns.lmplot(\n",
    "    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n",
    "    hue='Topic',\n",
    "    palette=sns.color_palette(\"hls\", NUM_TOPICS),\n",
    "    data=tsnedf,\n",
    "    legend=\"full\",\n",
    "    fit_reg=False\n",
    ")\n",
    "plt.axis('Off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-bibliography",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train and Deploy the Content Recommendation Model using KNN\n",
    "\n",
    "# training with KNN has 3 steps:\n",
    "# 1. sampling            - reducing the size of the initial dataset to fit into memory\n",
    "# 2. dimension reduction - reduce feature dimension of the data to decrease the model's\n",
    "#                          memory and inference latency\n",
    "# 3. index building      - enable efficient lookups of distance between data points whose\n",
    "#                          labels have not been determined yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-employer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.a. Run the training job\n",
    "\n",
    "# link the shuffled labels to the original labels in the training data\n",
    "labels = newidx \n",
    "labeldict = dict(zip(newidx,idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-douglas",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the training data in S3\n",
    "\n",
    "import io\n",
    "import sagemaker.amazon.common as smac\n",
    "\n",
    "print('train_features shape = ', predictions.shape)\n",
    "print('train_labels shape = ', labels.shape)\n",
    "buf = io.BytesIO()\n",
    "smac.write_numpy_to_dense_tensor(buf, predictions, labels)\n",
    "buf.seek(0)\n",
    "\n",
    "bucket = BUCKET\n",
    "prefix = PREFIX\n",
    "key = 'knn/train'\n",
    "fname = os.path.join(prefix, key)\n",
    "print(fname)\n",
    "boto3.resource('s3').Bucket(bucket).Object(fname).upload_fileobj(buf)\n",
    "s3_train_data = 's3://{}/{}/{}'.format(bucket, prefix, key)\n",
    "print('uploaded training data location: {}'.format(s3_train_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-gregory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a SageMaker Estimator for the KNN algorithm\n",
    "\n",
    "def trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path, s3_test_data=None):\n",
    "    \"\"\"\n",
    "    Create an Estimator from the given hyperparams, fit to training data, \n",
    "    and return a deployed predictor\n",
    "    \n",
    "    \"\"\"\n",
    "    # set up the estimator\n",
    "    knn = sagemaker.estimator.Estimator(get_image_uri(boto3.Session().region_name, \"knn\"),\n",
    "        get_execution_role(),\n",
    "        train_instance_count=1,\n",
    "        train_instance_type='ml.c4.xlarge',\n",
    "        output_path=output_path,\n",
    "        sagemaker_session=sagemaker.Session())\n",
    "    knn.set_hyperparameters(**hyperparams)\n",
    "    \n",
    "    # train a model. fit_input contains the locations of the train and test data\n",
    "    fit_input = {'train': s3_train_data}\n",
    "    knn.fit(fit_input)\n",
    "    return knn\n",
    "\n",
    "hyperparams = {\n",
    "    'feature_dim': predictions.shape[1],\n",
    "    'k': NUM_NEIGHBORS,\n",
    "    'sample_size': predictions.shape[0],\n",
    "    'predictor_type': 'classifier' ,\n",
    "    'index_metric':'COSINE'\n",
    "}\n",
    "\n",
    "# COSINE distance = (A (dot product) B) / (||A|| * ||B||),\n",
    "# where A and B are vectors\n",
    "\n",
    "# the default distance for KNN is Euclidean distance\n",
    "\n",
    "output_path = 's3://' + bucket + '/' + prefix + '/knn/output'\n",
    "knn_estimator = trained_estimator_from_hyperparams(s3_train_data, hyperparams, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-housing",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.b. Deploy the Model as a Hosted Endpoint\n",
    "\n",
    "def predictor_from_estimator(knn_estimator, estimator_name, instance_type, endpoint_name=None): \n",
    "    knn_predictor = knn_estimator.deploy(initial_instance_count=1, instance_type=instance_type,\n",
    "                                        endpoint_name=endpoint_name,\n",
    "                                        accept=\"application/jsonlines; verbose=true\")\n",
    "    knn_predictor.content_type = 'text/csv'\n",
    "    knn_predictor.serializer = csv_serializer\n",
    "    knn_predictor.deserializer = json_deserializer\n",
    "    return knn_predictor\n",
    "\n",
    "import time\n",
    "instance_type = 'ml.m4.xlarge'\n",
    "model_name = 'knn_%s'% instance_type\n",
    "endpoint_name = 'knn-ml-m4-xlarge-%s'% (str(time.time()).replace('.','-'))\n",
    "print('setting up the endpoint..')\n",
    "knn_predictor = predictor_from_estimator(knn_estimator, model_name, instance_type, endpoint_name=endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polyphonic-invitation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess the test data for running inferences\n",
    "\n",
    "def preprocess_input(text):\n",
    "    text = strip_newsgroup_header(text)\n",
    "    text = strip_newsgroup_quoting(text)\n",
    "    text = strip_newsgroup_footer(text)\n",
    "    return text    \n",
    "    \n",
    "test_data_prep = []\n",
    "for i in range(len(newsgroups_test)):\n",
    "    test_data_prep.append(preprocess_input(newsgroups_test[i]))\n",
    "test_vectors = vectorizer.fit_transform(test_data_prep)\n",
    "\n",
    "test_vectors = np.array(test_vectors.todense())\n",
    "test_topics = []\n",
    "for vec in test_vectors:\n",
    "    test_result = ntm_predictor.predict(vec)\n",
    "    test_topics.append(test_result['predictions'][0]['topic_weights'])\n",
    "\n",
    "topic_predictions = []\n",
    "for topic in test_topics:\n",
    "    result = knn_predictor.predict(topic)\n",
    "    cur_predictions = np.array([int(result['labels'][i]) for i in range(len(result['labels']))])\n",
    "    topic_predictions.append(cur_predictions[::-1][:10])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liquid-violence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.c. Explore Content Recommendation Model\n",
    "\n",
    "# plot the topic distribution of a specified topic, and compare it\n",
    "# with the closest k topics recommended by the model\n",
    "def plot_topic_distribution(topic_num, k = 5):\n",
    "    \n",
    "    closest_topics = [predictions[labeldict[x]] for x in topic_predictions[topic_num][:k]]\n",
    "    closest_topics.append(np.array(test_topics[topic_num]))\n",
    "    closest_topics = np.array(closest_topics)\n",
    "    df = pd.DataFrame(closest_topics.T)\n",
    "    df.rename(columns ={k:\"Test Document Distribution\"}, inplace=True)\n",
    "    fs = 12\n",
    "    df.plot(kind='bar', figsize=(16,4), fontsize=fs)\n",
    "    plt.ylabel('Topic assignment', fontsize=fs+2)\n",
    "    plt.xlabel('Topic ID', fontsize=fs+2)\n",
    "    plt.show()\n",
    "    \n",
    "# plot the topic distribution\n",
    "plot_topic_distribution(18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "packed-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another topic\n",
    "plot_topic_distribution(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "progressive-gambling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# another topic\n",
    "plot_topic_distribution(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "domestic-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a trend should be that the test topic distribution should be similar to \n",
    "# the k closest topics recommended by the model\n",
    "\n",
    "# suggests that for semantic retrieval task, a good approach is first vectorizing the topics,\n",
    "# and serving them with a KNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Delete Resources\n",
    "\n",
    "# 4.a. Endpoints\n",
    "ntm_predictor.delete_endpoint()\n",
    "knn_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}